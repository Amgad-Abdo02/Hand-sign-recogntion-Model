{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imported libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import copy\n",
    "import itertools\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A function that uses Tensorlfow lite model to classify hand sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KeyPointClassifier(landmark_list):\n",
    "     model_path='model/Lite_model.tflite'\n",
    "     num_threads=1\n",
    "\n",
    "     # Intializing Tensorflow lite model\n",
    "     interpreter =tf.lite.Interpreter(model_path=model_path,\n",
    "                                               num_threads=num_threads)\n",
    "     interpreter.allocate_tensors()\n",
    "     input_details = interpreter.get_input_details()\n",
    "     output_details = interpreter.get_output_details()\n",
    "\n",
    "     # Setting input tensors and entering input data\n",
    "     input_details_tensor_index = input_details[0]['index']\n",
    "     interpreter.set_tensor(\n",
    "            input_details_tensor_index,\n",
    "            np.array([landmark_list], dtype=np.float32))\n",
    "     \n",
    "     # Running inference model\n",
    "     interpreter.invoke()\n",
    "     \n",
    "     # Extracting output label from model\n",
    "     output_details_tensor_index = output_details[0]['index']\n",
    "     result = interpreter.get_tensor(output_details_tensor_index)\n",
    "     result_index = np.argmax(np.squeeze(result))\n",
    "\n",
    "     return result_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A function reads from user to change modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_mode(key, mode):\n",
    "    number = -1\n",
    "    if 48 <= key <= 57:  # 0 to 9\n",
    "        number = key - 48\n",
    "    if key == 118:  # v\n",
    "        mode = 0 # Normal Camera mode\n",
    "    if key == 99:  # c\n",
    "        mode = 1 # Data capturing mode\n",
    "    return number, mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to Calculate Landmark list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmark_list(image,handslms,handedness):\n",
    "    image_width, image_height = image.shape[1], image.shape[0]\n",
    "\n",
    "    landmark_point = []\n",
    "    \n",
    "    mp_draw = mp.solutions.drawing_utils\n",
    "    mp_hands = mp.solutions.hands\n",
    "    # Keypoint\n",
    "    for _, landmark in enumerate(handslms.landmark):\n",
    "        landmark_x = int(landmark.x * image_width)\n",
    "        landmark_y = int(landmark.y * image_height)\n",
    "\n",
    "        landmark_point.append([landmark_x, landmark_y])\n",
    "\n",
    "        if handedness.classification[0].label == \"Left\":\n",
    "            color = (0,0,255)\n",
    "        else:\n",
    "            color = (255,0,255)\n",
    "    \n",
    "    mp_draw.draw_landmarks(image,handslms,mp_hands.HAND_CONNECTIONS,\n",
    "                           landmark_drawing_spec=mp_draw.DrawingSpec(color =color))\n",
    "    return np.array(landmark_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to normalize data to feed into model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_landmark(landmark_list):\n",
    "    temp_landmark_list = copy.deepcopy(landmark_list)\n",
    "\n",
    "    # Convert to relative coordinates\n",
    "    base_x, base_y = 0, 0\n",
    "    for index, landmark_point in enumerate(temp_landmark_list):\n",
    "        if index == 0:\n",
    "            base_x, base_y = landmark_point[0], landmark_point[1]\n",
    "\n",
    "        temp_landmark_list[index][0] = temp_landmark_list[index][0] - base_x\n",
    "        temp_landmark_list[index][1] = temp_landmark_list[index][1] - base_y\n",
    "\n",
    "    # Convert to a one-dimensional list\n",
    "    temp_landmark_list = list(\n",
    "        itertools.chain.from_iterable(temp_landmark_list))\n",
    "\n",
    "    # Normalization\n",
    "    max_value = max(list(map(abs, temp_landmark_list)))\n",
    "\n",
    "    def normalize_(n):\n",
    "        return n / max_value\n",
    "\n",
    "    temp_landmark_list = list(map(normalize_, temp_landmark_list))\n",
    "\n",
    "    return temp_landmark_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to Create data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logging_csv(number, mode, landmark_list):\n",
    "    if mode == 0:\n",
    "        pass\n",
    "    if mode == 1 and (0 <= number <= 9):\n",
    "        csv_path = 'Dataset/keypoint.csv'\n",
    "        with open(csv_path, 'a', newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([number, *landmark_list])\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to draw Rectangle around hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bounding_rect(use_rect, image, rect):\n",
    "    if use_rect:\n",
    "        # Outer rectangle\n",
    "        cv.rectangle(image, (rect[0], rect[1]), (rect[2], rect[3]),\n",
    "                     (0, 0, 0), 1)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to write sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_info_text(image, rect, handedness, hand_sign_text):\n",
    "    cv.rectangle(image, (rect[0], rect[1]), (rect[2], rect[1] - 22),\n",
    "                 (0, 0, 0), -1)\n",
    "\n",
    "    info_text = handedness.classification[0].label[0:]\n",
    "    if hand_sign_text != \"\":\n",
    "        info_text = info_text + ':' + hand_sign_text\n",
    "    cv.putText(image, info_text, (rect[0] + 5, rect[1] - 4),\n",
    "               cv.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1, cv.LINE_AA)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to write mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_info(image,mode, number):\n",
    "    mode_string = ['Logging Key Point', 'Logging Point History']\n",
    "    if 1 == mode:\n",
    "        cv.putText(image, \"MODE:\" + mode_string[mode - 1], (10, 90),\n",
    "                   cv.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1,\n",
    "                   cv.LINE_AA)\n",
    "        if 0 <= number <= 9:\n",
    "            cv.putText(image, \"NUM:\" + str(number), (10, 110),\n",
    "                       cv.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1,\n",
    "                       cv.LINE_AA)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model for Sign recogntion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetching  and splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'Dataset/keypoint.csv'\n",
    "model_save_path = 'model/Lite_model.hdf5'\n",
    "\n",
    "X_dataset = np.loadtxt(dataset, delimiter=',', dtype='float32', usecols=list(range(1, (21 * 2) + 1)))\n",
    "y_dataset = np.loadtxt(dataset, delimiter=',', dtype='int32', usecols=(0))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_dataset, y_dataset, train_size=0.75, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intiatlizing, compiling and training model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dropout (Dropout)           (None, 42)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 40)                1720      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 40)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 60)                2460      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 60)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 20)                1220      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 8)                 168       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,568\n",
      "Trainable params: 5,568\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = 8\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input((21 * 2, )),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(40, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(60, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(20, activation='relu'),\n",
    "    tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving checkpoints of model\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    model_save_path, verbose=1, save_weights_only=False)\n",
    "\n",
    "# Early stopping if model doesn't improve for 20 epochs\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(patience=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/60 [=========================>....] - ETA: 0s - loss: 2.0021 - accuracy: 0.2362\n",
      "Epoch 1: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 2s 14ms/step - loss: 1.9921 - accuracy: 0.2375 - val_loss: 1.8427 - val_accuracy: 0.2694\n",
      "Epoch 2/1000\n",
      "51/60 [========================>.....] - ETA: 0s - loss: 1.7959 - accuracy: 0.3039\n",
      "Epoch 2: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 1.7787 - accuracy: 0.3096 - val_loss: 1.5086 - val_accuracy: 0.3578\n",
      "Epoch 3/1000\n",
      "59/60 [============================>.] - ETA: 0s - loss: 1.5620 - accuracy: 0.3774\n",
      "Epoch 3: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.5621 - accuracy: 0.3770 - val_loss: 1.2915 - val_accuracy: 0.5039\n",
      "Epoch 4/1000\n",
      "55/60 [==========================>...] - ETA: 0s - loss: 1.4093 - accuracy: 0.4278\n",
      "Epoch 4: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.4080 - accuracy: 0.4271 - val_loss: 1.1188 - val_accuracy: 0.6769\n",
      "Epoch 5/1000\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 1.3003 - accuracy: 0.4586\n",
      "Epoch 5: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 1.3001 - accuracy: 0.4589 - val_loss: 0.9796 - val_accuracy: 0.6912\n",
      "Epoch 6/1000\n",
      "60/60 [==============================] - ETA: 0s - loss: 1.2219 - accuracy: 0.5029\n",
      "Epoch 6: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 1.2219 - accuracy: 0.5029 - val_loss: 0.9035 - val_accuracy: 0.6975\n",
      "Epoch 7/1000\n",
      "60/60 [==============================] - ETA: 0s - loss: 1.1714 - accuracy: 0.5266\n",
      "Epoch 7: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 12ms/step - loss: 1.1714 - accuracy: 0.5266 - val_loss: 0.8140 - val_accuracy: 0.7520\n",
      "Epoch 8/1000\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 1.1457 - accuracy: 0.5351\n",
      "Epoch 8: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 1.1409 - accuracy: 0.5377 - val_loss: 0.7777 - val_accuracy: 0.8191\n",
      "Epoch 9/1000\n",
      "54/60 [==========================>...] - ETA: 0s - loss: 1.0886 - accuracy: 0.5472\n",
      "Epoch 9: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0907 - accuracy: 0.5487 - val_loss: 0.7383 - val_accuracy: 0.8381\n",
      "Epoch 10/1000\n",
      "54/60 [==========================>...] - ETA: 0s - loss: 1.0560 - accuracy: 0.5799\n",
      "Epoch 10: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 1.0555 - accuracy: 0.5798 - val_loss: 0.6895 - val_accuracy: 0.8325\n",
      "Epoch 11/1000\n",
      "59/60 [============================>.] - ETA: 0s - loss: 1.0415 - accuracy: 0.5808\n",
      "Epoch 11: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 1.0427 - accuracy: 0.5803 - val_loss: 0.6644 - val_accuracy: 0.8831\n",
      "Epoch 12/1000\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 1.0050 - accuracy: 0.5971\n",
      "Epoch 12: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 1.0074 - accuracy: 0.5969 - val_loss: 0.6388 - val_accuracy: 0.8570\n",
      "Epoch 13/1000\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 0.9612 - accuracy: 0.6239\n",
      "Epoch 13: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.9639 - accuracy: 0.6211 - val_loss: 0.5843 - val_accuracy: 0.8681\n",
      "Epoch 14/1000\n",
      "55/60 [==========================>...] - ETA: 0s - loss: 0.9846 - accuracy: 0.6011\n",
      "Epoch 14: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.9863 - accuracy: 0.6024 - val_loss: 0.5896 - val_accuracy: 0.8381\n",
      "Epoch 15/1000\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 0.9577 - accuracy: 0.6334\n",
      "Epoch 15: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.9523 - accuracy: 0.6351 - val_loss: 0.5711 - val_accuracy: 0.8705\n",
      "Epoch 16/1000\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.9456 - accuracy: 0.6285\n",
      "Epoch 16: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.9456 - accuracy: 0.6285 - val_loss: 0.5489 - val_accuracy: 0.8949\n",
      "Epoch 17/1000\n",
      "54/60 [==========================>...] - ETA: 0s - loss: 0.9320 - accuracy: 0.6377\n",
      "Epoch 17: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.9362 - accuracy: 0.6361 - val_loss: 0.5330 - val_accuracy: 0.8894\n",
      "Epoch 18/1000\n",
      "54/60 [==========================>...] - ETA: 0s - loss: 0.8839 - accuracy: 0.6638\n",
      "Epoch 18: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 12ms/step - loss: 0.8883 - accuracy: 0.6601 - val_loss: 0.5115 - val_accuracy: 0.8847\n",
      "Epoch 19/1000\n",
      "55/60 [==========================>...] - ETA: 0s - loss: 0.9188 - accuracy: 0.6509\n",
      "Epoch 19: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 0.9103 - accuracy: 0.6535 - val_loss: 0.5107 - val_accuracy: 0.8626\n",
      "Epoch 20/1000\n",
      "53/60 [=========================>....] - ETA: 0s - loss: 0.8797 - accuracy: 0.6554\n",
      "Epoch 20: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 0.8755 - accuracy: 0.6569 - val_loss: 0.4880 - val_accuracy: 0.8989\n",
      "Epoch 21/1000\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 0.8776 - accuracy: 0.6638\n",
      "Epoch 21: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 12ms/step - loss: 0.8741 - accuracy: 0.6638 - val_loss: 0.4764 - val_accuracy: 0.9186\n",
      "Epoch 22/1000\n",
      "55/60 [==========================>...] - ETA: 0s - loss: 0.8493 - accuracy: 0.6724\n",
      "Epoch 22: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.8445 - accuracy: 0.6761 - val_loss: 0.4473 - val_accuracy: 0.9123\n",
      "Epoch 23/1000\n",
      "53/60 [=========================>....] - ETA: 0s - loss: 0.8335 - accuracy: 0.6828\n",
      "Epoch 23: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.8314 - accuracy: 0.6804 - val_loss: 0.4428 - val_accuracy: 0.9068\n",
      "Epoch 24/1000\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 0.8289 - accuracy: 0.6766\n",
      "Epoch 24: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 0.8302 - accuracy: 0.6764 - val_loss: 0.4398 - val_accuracy: 0.9155\n",
      "Epoch 25/1000\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.8285 - accuracy: 0.6891\n",
      "Epoch 25: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 0.8289 - accuracy: 0.6896 - val_loss: 0.4404 - val_accuracy: 0.8847\n",
      "Epoch 26/1000\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.8189 - accuracy: 0.6862\n",
      "Epoch 26: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 0.8191 - accuracy: 0.6867 - val_loss: 0.4174 - val_accuracy: 0.9305\n",
      "Epoch 27/1000\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.8167 - accuracy: 0.6894\n",
      "Epoch 27: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.8153 - accuracy: 0.6901 - val_loss: 0.4129 - val_accuracy: 0.8863\n",
      "Epoch 28/1000\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.7890 - accuracy: 0.6947\n",
      "Epoch 28: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.7892 - accuracy: 0.6946 - val_loss: 0.4110 - val_accuracy: 0.9258\n",
      "Epoch 29/1000\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.7893 - accuracy: 0.6923\n",
      "Epoch 29: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.7891 - accuracy: 0.6930 - val_loss: 0.3892 - val_accuracy: 0.9408\n",
      "Epoch 30/1000\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.7687 - accuracy: 0.7071\n",
      "Epoch 30: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.7708 - accuracy: 0.7064 - val_loss: 0.3828 - val_accuracy: 0.9447\n",
      "Epoch 31/1000\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.7536 - accuracy: 0.7193\n",
      "Epoch 31: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 0.7536 - accuracy: 0.7193 - val_loss: 0.3669 - val_accuracy: 0.9471\n",
      "Epoch 32/1000\n",
      "52/60 [=========================>....] - ETA: 0s - loss: 0.7658 - accuracy: 0.7055\n",
      "Epoch 32: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 0.7625 - accuracy: 0.7062 - val_loss: 0.3724 - val_accuracy: 0.9336\n",
      "Epoch 33/1000\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.7610 - accuracy: 0.7169\n",
      "Epoch 33: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.7592 - accuracy: 0.7172 - val_loss: 0.3744 - val_accuracy: 0.9344\n",
      "Epoch 34/1000\n",
      "55/60 [==========================>...] - ETA: 0s - loss: 0.7394 - accuracy: 0.7165\n",
      "Epoch 34: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.7378 - accuracy: 0.7159 - val_loss: 0.3475 - val_accuracy: 0.9573\n",
      "Epoch 35/1000\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.7278 - accuracy: 0.7309\n",
      "Epoch 35: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.7278 - accuracy: 0.7309 - val_loss: 0.3299 - val_accuracy: 0.9566\n",
      "Epoch 36/1000\n",
      "54/60 [==========================>...] - ETA: 0s - loss: 0.7226 - accuracy: 0.7190\n",
      "Epoch 36: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.7266 - accuracy: 0.7167 - val_loss: 0.3334 - val_accuracy: 0.9526\n",
      "Epoch 37/1000\n",
      "54/60 [==========================>...] - ETA: 0s - loss: 0.7220 - accuracy: 0.7295\n",
      "Epoch 37: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.7173 - accuracy: 0.7299 - val_loss: 0.3336 - val_accuracy: 0.9439\n",
      "Epoch 38/1000\n",
      "55/60 [==========================>...] - ETA: 0s - loss: 0.7371 - accuracy: 0.7224\n",
      "Epoch 38: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 0.7396 - accuracy: 0.7220 - val_loss: 0.3450 - val_accuracy: 0.9115\n",
      "Epoch 39/1000\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.7063 - accuracy: 0.7383\n",
      "Epoch 39: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 0.7063 - accuracy: 0.7383 - val_loss: 0.3430 - val_accuracy: 0.9384\n",
      "Epoch 40/1000\n",
      "54/60 [==========================>...] - ETA: 0s - loss: 0.7248 - accuracy: 0.7292\n",
      "Epoch 40: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.7220 - accuracy: 0.7317 - val_loss: 0.3408 - val_accuracy: 0.9455\n",
      "Epoch 41/1000\n",
      "53/60 [=========================>....] - ETA: 0s - loss: 0.7155 - accuracy: 0.7400\n",
      "Epoch 41: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.7171 - accuracy: 0.7370 - val_loss: 0.3245 - val_accuracy: 0.9597\n",
      "Epoch 42/1000\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.6967 - accuracy: 0.7443\n",
      "Epoch 42: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 12ms/step - loss: 0.6967 - accuracy: 0.7443 - val_loss: 0.3208 - val_accuracy: 0.9613\n",
      "Epoch 43/1000\n",
      "55/60 [==========================>...] - ETA: 0s - loss: 0.6917 - accuracy: 0.7401\n",
      "Epoch 43: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 12ms/step - loss: 0.6928 - accuracy: 0.7396 - val_loss: 0.3248 - val_accuracy: 0.9447\n",
      "Epoch 44/1000\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.6897 - accuracy: 0.7449\n",
      "Epoch 44: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.6897 - accuracy: 0.7449 - val_loss: 0.3194 - val_accuracy: 0.9558\n",
      "Epoch 45/1000\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.6847 - accuracy: 0.7407\n",
      "Epoch 45: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.6847 - accuracy: 0.7401 - val_loss: 0.3065 - val_accuracy: 0.9502\n",
      "Epoch 46/1000\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.7025 - accuracy: 0.7336\n",
      "Epoch 46: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.7007 - accuracy: 0.7349 - val_loss: 0.3287 - val_accuracy: 0.9581\n",
      "Epoch 47/1000\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.6741 - accuracy: 0.7474\n",
      "Epoch 47: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.6751 - accuracy: 0.7467 - val_loss: 0.3269 - val_accuracy: 0.9344\n",
      "Epoch 48/1000\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.6786 - accuracy: 0.7478\n",
      "Epoch 48: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.6786 - accuracy: 0.7478 - val_loss: 0.3010 - val_accuracy: 0.9613\n",
      "Epoch 49/1000\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 0.6796 - accuracy: 0.7430\n",
      "Epoch 49: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.6865 - accuracy: 0.7412 - val_loss: 0.3147 - val_accuracy: 0.9502\n",
      "Epoch 50/1000\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.6692 - accuracy: 0.7423\n",
      "Epoch 50: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.6697 - accuracy: 0.7420 - val_loss: 0.3112 - val_accuracy: 0.9408\n",
      "Epoch 51/1000\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.6753 - accuracy: 0.7467\n",
      "Epoch 51: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 13ms/step - loss: 0.6753 - accuracy: 0.7467 - val_loss: 0.3086 - val_accuracy: 0.9566\n",
      "Epoch 52/1000\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.6513 - accuracy: 0.7526\n",
      "Epoch 52: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.6505 - accuracy: 0.7525 - val_loss: 0.2859 - val_accuracy: 0.9573\n",
      "Epoch 53/1000\n",
      "53/60 [=========================>....] - ETA: 0s - loss: 0.6356 - accuracy: 0.7535\n",
      "Epoch 53: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.6436 - accuracy: 0.7512 - val_loss: 0.2783 - val_accuracy: 0.9621\n",
      "Epoch 54/1000\n",
      "54/60 [==========================>...] - ETA: 0s - loss: 0.6452 - accuracy: 0.7685\n",
      "Epoch 54: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.6457 - accuracy: 0.7657 - val_loss: 0.2776 - val_accuracy: 0.9637\n",
      "Epoch 55/1000\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.6493 - accuracy: 0.7562\n",
      "Epoch 55: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.6515 - accuracy: 0.7551 - val_loss: 0.2907 - val_accuracy: 0.9637\n",
      "Epoch 56/1000\n",
      "55/60 [==========================>...] - ETA: 0s - loss: 0.6356 - accuracy: 0.7608\n",
      "Epoch 56: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.6303 - accuracy: 0.7630 - val_loss: 0.2882 - val_accuracy: 0.9558\n",
      "Epoch 57/1000\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 0.6197 - accuracy: 0.7744\n",
      "Epoch 57: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.6201 - accuracy: 0.7746 - val_loss: 0.2625 - val_accuracy: 0.9550\n",
      "Epoch 58/1000\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 0.6244 - accuracy: 0.7637\n",
      "Epoch 58: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 0.6229 - accuracy: 0.7657 - val_loss: 0.2802 - val_accuracy: 0.9566\n",
      "Epoch 59/1000\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.6416 - accuracy: 0.7581\n",
      "Epoch 59: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.6460 - accuracy: 0.7572 - val_loss: 0.2767 - val_accuracy: 0.9621\n",
      "Epoch 60/1000\n",
      "54/60 [==========================>...] - ETA: 0s - loss: 0.6427 - accuracy: 0.7622\n",
      "Epoch 60: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.6469 - accuracy: 0.7588 - val_loss: 0.2805 - val_accuracy: 0.9573\n",
      "Epoch 61/1000\n",
      "54/60 [==========================>...] - ETA: 0s - loss: 0.6305 - accuracy: 0.7694\n",
      "Epoch 61: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.6202 - accuracy: 0.7704 - val_loss: 0.2769 - val_accuracy: 0.9487\n",
      "Epoch 62/1000\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.6091 - accuracy: 0.7738\n",
      "Epoch 62: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.6091 - accuracy: 0.7738 - val_loss: 0.2476 - val_accuracy: 0.9660\n",
      "Epoch 63/1000\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.6299 - accuracy: 0.7722\n",
      "Epoch 63: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.6293 - accuracy: 0.7722 - val_loss: 0.2698 - val_accuracy: 0.9463\n",
      "Epoch 64/1000\n",
      "51/60 [========================>.....] - ETA: 0s - loss: 0.6244 - accuracy: 0.7619\n",
      "Epoch 64: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.6256 - accuracy: 0.7604 - val_loss: 0.2611 - val_accuracy: 0.9676\n",
      "Epoch 65/1000\n",
      "51/60 [========================>.....] - ETA: 0s - loss: 0.6287 - accuracy: 0.7672\n",
      "Epoch 65: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.6284 - accuracy: 0.7657 - val_loss: 0.2719 - val_accuracy: 0.9455\n",
      "Epoch 66/1000\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.5968 - accuracy: 0.7712\n",
      "Epoch 66: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.5988 - accuracy: 0.7709 - val_loss: 0.2721 - val_accuracy: 0.9629\n",
      "Epoch 67/1000\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 0.6026 - accuracy: 0.7776\n",
      "Epoch 67: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 0.6020 - accuracy: 0.7759 - val_loss: 0.2676 - val_accuracy: 0.9613\n",
      "Epoch 68/1000\n",
      "55/60 [==========================>...] - ETA: 0s - loss: 0.5960 - accuracy: 0.7739\n",
      "Epoch 68: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 8ms/step - loss: 0.5937 - accuracy: 0.7741 - val_loss: 0.2679 - val_accuracy: 0.9676\n",
      "Epoch 69/1000\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.5878 - accuracy: 0.7780\n",
      "Epoch 69: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 8ms/step - loss: 0.5894 - accuracy: 0.7770 - val_loss: 0.2520 - val_accuracy: 0.9645\n",
      "Epoch 70/1000\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.5773 - accuracy: 0.7864\n",
      "Epoch 70: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 13ms/step - loss: 0.5779 - accuracy: 0.7854 - val_loss: 0.2578 - val_accuracy: 0.9645\n",
      "Epoch 71/1000\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 0.6030 - accuracy: 0.7782\n",
      "Epoch 71: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 8ms/step - loss: 0.6026 - accuracy: 0.7783 - val_loss: 0.2459 - val_accuracy: 0.9684\n",
      "Epoch 72/1000\n",
      "54/60 [==========================>...] - ETA: 0s - loss: 0.6090 - accuracy: 0.7746\n",
      "Epoch 72: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 0.6072 - accuracy: 0.7736 - val_loss: 0.2610 - val_accuracy: 0.9621\n",
      "Epoch 73/1000\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 0.5796 - accuracy: 0.7867\n",
      "Epoch 73: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 12ms/step - loss: 0.5869 - accuracy: 0.7846 - val_loss: 0.2428 - val_accuracy: 0.9692\n",
      "Epoch 74/1000\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 0.5785 - accuracy: 0.7818\n",
      "Epoch 74: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 0.5806 - accuracy: 0.7807 - val_loss: 0.2503 - val_accuracy: 0.9708\n",
      "Epoch 75/1000\n",
      "55/60 [==========================>...] - ETA: 0s - loss: 0.5791 - accuracy: 0.7812\n",
      "Epoch 75: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.5851 - accuracy: 0.7796 - val_loss: 0.2666 - val_accuracy: 0.9621\n",
      "Epoch 76/1000\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.5863 - accuracy: 0.7802\n",
      "Epoch 76: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.5852 - accuracy: 0.7807 - val_loss: 0.2652 - val_accuracy: 0.9645\n",
      "Epoch 77/1000\n",
      "54/60 [==========================>...] - ETA: 0s - loss: 0.5887 - accuracy: 0.7810\n",
      "Epoch 77: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.5907 - accuracy: 0.7791 - val_loss: 0.2502 - val_accuracy: 0.9645\n",
      "Epoch 78/1000\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.5785 - accuracy: 0.7839\n",
      "Epoch 78: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 12ms/step - loss: 0.5783 - accuracy: 0.7844 - val_loss: 0.2660 - val_accuracy: 0.9668\n",
      "Epoch 79/1000\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.5711 - accuracy: 0.7883\n",
      "Epoch 79: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.5686 - accuracy: 0.7888 - val_loss: 0.2636 - val_accuracy: 0.9581\n",
      "Epoch 80/1000\n",
      "54/60 [==========================>...] - ETA: 0s - loss: 0.5671 - accuracy: 0.7954\n",
      "Epoch 80: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.5670 - accuracy: 0.7967 - val_loss: 0.2512 - val_accuracy: 0.9629\n",
      "Epoch 81/1000\n",
      "52/60 [=========================>....] - ETA: 0s - loss: 0.6096 - accuracy: 0.7752\n",
      "Epoch 81: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 8ms/step - loss: 0.6047 - accuracy: 0.7767 - val_loss: 0.2684 - val_accuracy: 0.9684\n",
      "Epoch 82/1000\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.5763 - accuracy: 0.7901\n",
      "Epoch 82: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.5751 - accuracy: 0.7902 - val_loss: 0.2757 - val_accuracy: 0.9637\n",
      "Epoch 83/1000\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 0.5440 - accuracy: 0.8050\n",
      "Epoch 83: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.5455 - accuracy: 0.8038 - val_loss: 0.2515 - val_accuracy: 0.9716\n",
      "Epoch 84/1000\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.5786 - accuracy: 0.7863\n",
      "Epoch 84: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.5780 - accuracy: 0.7862 - val_loss: 0.2605 - val_accuracy: 0.9660\n",
      "Epoch 85/1000\n",
      "54/60 [==========================>...] - ETA: 0s - loss: 0.5475 - accuracy: 0.7957\n",
      "Epoch 85: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 0.5549 - accuracy: 0.7923 - val_loss: 0.2512 - val_accuracy: 0.9597\n",
      "Epoch 86/1000\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.5417 - accuracy: 0.7996\n",
      "Epoch 86: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 0.5417 - accuracy: 0.7996 - val_loss: 0.2560 - val_accuracy: 0.9566\n",
      "Epoch 87/1000\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.5589 - accuracy: 0.7909\n",
      "Epoch 87: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 0.5589 - accuracy: 0.7909 - val_loss: 0.2443 - val_accuracy: 0.9550\n",
      "Epoch 88/1000\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.5697 - accuracy: 0.7886\n",
      "Epoch 88: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 12ms/step - loss: 0.5697 - accuracy: 0.7886 - val_loss: 0.2709 - val_accuracy: 0.9581\n",
      "Epoch 89/1000\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 0.5497 - accuracy: 0.7947\n",
      "Epoch 89: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 0.5478 - accuracy: 0.7949 - val_loss: 0.2663 - val_accuracy: 0.9494\n",
      "Epoch 90/1000\n",
      "55/60 [==========================>...] - ETA: 0s - loss: 0.5637 - accuracy: 0.7855\n",
      "Epoch 90: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 15ms/step - loss: 0.5602 - accuracy: 0.7878 - val_loss: 0.2464 - val_accuracy: 0.9652\n",
      "Epoch 91/1000\n",
      "55/60 [==========================>...] - ETA: 0s - loss: 0.5530 - accuracy: 0.7918\n",
      "Epoch 91: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 12ms/step - loss: 0.5533 - accuracy: 0.7912 - val_loss: 0.2585 - val_accuracy: 0.9597\n",
      "Epoch 92/1000\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 0.5470 - accuracy: 0.7963\n",
      "Epoch 92: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 13ms/step - loss: 0.5503 - accuracy: 0.7959 - val_loss: 0.2586 - val_accuracy: 0.9542\n",
      "Epoch 93/1000\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 0.5270 - accuracy: 0.7952\n",
      "Epoch 93: saving model to model\\Lite_model.hdf5\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 0.5290 - accuracy: 0.7946 - val_loss: 0.2446 - val_accuracy: 0.9605\n",
      "Epoch 93: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20a05894430>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=1000,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[cp_callback, es_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation using confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.94      0.97       181\n",
      "           1       0.99      0.83      0.90       143\n",
      "           2       0.99      0.99      0.99       157\n",
      "           3       0.96      0.97      0.97       278\n",
      "           4       0.85      1.00      0.92        94\n",
      "           5       0.96      1.00      0.98       103\n",
      "           6       0.94      0.98      0.96       180\n",
      "           7       0.98      0.98      0.98       130\n",
      "\n",
      "    accuracy                           0.96      1266\n",
      "   macro avg       0.96      0.96      0.96      1266\n",
      "weighted avg       0.96      0.96      0.96      1266\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict(X_test)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "labels = sorted(list(set(y_test)))\n",
    "cmx_data = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "    \n",
    "df_cmx = pd.DataFrame(cmx_data, index=labels, columns=labels)\n",
    " \n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "sns.heatmap(df_cmx, annot=True, fmt='g' ,square=False)\n",
    "ax.set_ylim(len(set(y_test)), 0)\n",
    "plt.show()\n",
    "    \n",
    "\n",
    "print('Classification Report')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving model for tensorflow lite use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\BSBSME~1\\AppData\\Local\\Temp\\tmp46am9jpi\\assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save(model_save_path, include_optimizer=False)\n",
    "\n",
    "tflite_save_path = 'model/Lite_model.tflite'\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_quantized_model = converter.convert()\n",
    "\n",
    "open(tflite_save_path, 'wb').write(tflite_quantized_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting video capture configs\n",
    "cap = cv.VideoCapture(0)\n",
    "cap.set(cv.CAP_PROP_FRAME_WIDTH, 1920)\n",
    "cap.set(cv.CAP_PROP_FRAME_HEIGHT, 1080)\n",
    "\n",
    "# Loading Mediapipe model \n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(\n",
    "        static_image_mode= True,\n",
    "        max_num_hands=4, \n",
    "        min_detection_confidence=0.7,\n",
    "        min_tracking_confidence=0.5,\n",
    "    )\n",
    "\n",
    "#Reading Labels \n",
    "with open('Dataset/keypoint_classifier_label.csv',\n",
    "              encoding='utf-8-sig') as f:\n",
    "        keypoint_classifier_labels = csv.reader(f)\n",
    "        keypoint_classifier_labels = [\n",
    "            row[0] for row in keypoint_classifier_labels\n",
    "        ]\n",
    "\n",
    "mode = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "\n",
    "    key = cv.waitKey(10)\n",
    "    if key == 27:  # ESC\n",
    "        break\n",
    "\n",
    "    number,mode = select_mode(key,mode)\n",
    "\n",
    "    # Capturing Frame\n",
    "    ret, image = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "\n",
    "    # Flipping image for mediapipe\n",
    "    image = cv.flip(image, 1)\n",
    "\n",
    "    #copying image to draw on\n",
    "    debug_image = copy.deepcopy(image)\n",
    "    \n",
    "    image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "    #processing image to mediapipe to get hand landmarks\n",
    "    image.flags.writeable = False\n",
    "    results = hands.process(image)\n",
    "    image.flags.writeable = True\n",
    "\n",
    "    if results.multi_hand_landmarks is not None:\n",
    "        for hand_landmarks, handedness in zip(results.multi_hand_landmarks,\n",
    "                                                  results.multi_handedness):\n",
    "            # Calculating landmarks\n",
    "            landmark_list = draw_landmark_list(debug_image,hand_landmarks,handedness)\n",
    "\n",
    "            # Calculate Bounding Box\n",
    "            x, y, w, h = cv.boundingRect(landmark_list)\n",
    "            rect = [x, y, x + w, y + h]\n",
    "            \n",
    "            # Conversion to normalized coordinates\n",
    "            pre_processed_landmark_list = pre_process_landmark(\n",
    "                landmark_list)\n",
    "                        \n",
    "            # Write to the dataset file\n",
    "            logging_csv(number, mode, pre_processed_landmark_list)\n",
    "\n",
    "            # Hand sign classification\n",
    "            hand_sign_id = KeyPointClassifier(pre_processed_landmark_list)\n",
    "\n",
    "            # Drawing part\n",
    "            debug_image = draw_bounding_rect(True, debug_image, rect)\n",
    "            #debug_image = draw_landmarks(debug_image, landmark_list)\n",
    "            debug_image = draw_info_text(\n",
    "                debug_image,\n",
    "                rect,\n",
    "                handedness,\n",
    "                keypoint_classifier_labels[hand_sign_id],\n",
    "            )\n",
    "    debug_image = draw_info(debug_image, mode, number)\n",
    "\n",
    "    # Screen show\n",
    "    cv.imshow('Hand Gesture Recognition', debug_image)\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
